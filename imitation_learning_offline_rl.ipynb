{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "8pHtTfZKhfQJ",
        "330LSqoAh7BK",
        "orFDQCVch9Hn",
        "BPCYwEGrMbvd"
      ],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#  Imitation Learning & Offline RL\n",
        "*NB: Обучение на основе демонстраций (лекция 11).*\n",
        "\n",
        "Данное домашнее задание является продолжением семинара по Imitation Learning и Offline RL. Здесь мы будем переиспользовать наработки из семинара, а также приведенные в нем датасеты (*NB: далее в тетрадке все необходимое приведено!*).\n",
        "\n",
        "---\n",
        "\n",
        "На семинаре обсуждался метод Behavior Cloning (BC), который представляет собой подход к обучению моделей на основе пар “состояние-действие”, полученных от эксперта.\n",
        "\n",
        "Главное преимущество этого метода заключается в его простоте реализации. Однако у него есть два существенных недостатка:\n",
        "\n",
        "1. Накопление ошибок в ходе одного эпизода\n",
        "2. Сдвиг распределения состояний между обучающей и тестовой выборками — в результате обучения BC агент может предпочитать состояния, которые слабо представлены в экспертном датасете, следовательно точность экспертного решения может быть недостаточной.\n",
        "\n",
        "**Датасет** $\\mathcal{D} = \\{ (s, a, r, s')\\}$ собран с помощью неизвестной **экспертной стратегии** $\\pi_e(a | s)$. $s$ - состояние, $r$ - вознаграждение, $a$ - действие, $s'$ - следующее состояние. $s \\sim d^\\pi$ -- множество состояний в памяти.\n",
        "\n",
        "Цель: $\\max_\\pi \\sum\\limits_{t=0}^T \\mathbb{E}_{s_t \\sim d^\\pi, a_t \\sim \\pi(a | s)} \\left[\\gamma^t r(s_t, a_t)\\right]$\n",
        "\n",
        "## DAgger (Dataset Aggregation)\n",
        "\n",
        "[Статья](https://arxiv.org/abs/1011.0686).\n",
        "\n",
        "Задача: см. соотв. секцию в конце.\n",
        "\n",
        "Идея метода: собирать дополнительные экспертные данные во время обучения.\n",
        "\n",
        "Обозначения: $\\pi_a$ -- стратегия обучаемого агента, $\\pi_e$ -- стратегия эксперта, $\\mathcal{D}_e$ -- данные с эксперта, $M$ -- количество дополнительных данных.\n",
        "\n",
        "Алгоритм:\n",
        "1. Собрать новые данные $ \\{ s_i \\}_{i=0}^M$ с помощью **текущей** стратегии $\\pi_a$. *NB: в полученных траекториях нас интересуют только состояния.*\n",
        "2. Для собранного набора состояний опросить эксперта $\\pi_e$, какие действия выбрал бы он. В результате получен новый экспертный датасет $\\mathcal{D}_a = \\{(s_i, a_i)\\}_{i=0}^M$.\n",
        "3. Провести итерацию обучения агента $\\pi_a$ на расширенном наборе данных $\\mathcal{D} = \\mathcal{D}_e \\cup \\mathcal{D}_a$.\n",
        "4. Повторить.\n",
        "\n",
        "Таким образом, метод BC (behavior cloning) надо дополнить только итеративным способом обновления датасета.\n",
        "\n",
        "Основные особенности метода:\n",
        "- Помогает избежать накопления ошибок, возникающих при чистом копировании поведения (behavior cloning).\n",
        "- Охватывает те состояния, в которые агент действительно попадает во время выполнения задачи, что улучшает обобщающую способность модели.\n",
        "- Сводит задачу imitation learning к no-regret online learning. При разумных условиях DAgger гарантирует, что стратегия агента будет работать почти так же хорошо, как стратегия эксперта.\n",
        "\n",
        "Основные особенности/требования метода:\n",
        "- есть доступ к среде;\n",
        "- есть доступ к эксперту.\n",
        "\n",
        "### Код\n",
        "Что взять за основу:\n",
        "- `BCAgent`,\n",
        "- в качестве эксперта модель `SACAgent`, веса модели `expert.pt`.\n",
        "\n",
        "Что потребуется изменить:\n",
        "- метод `train`;\n",
        "- `ReplayBuffer`.\n",
        "\n",
        "Параметры:\n",
        "- $M$ -- количество данных за один update.\n",
        "- $k_e : k_a$ -- пропорция, сколько в батче экспертных данных к данным с агента (обычно $1 : 1$).\n",
        "\n",
        "Что можно попробовать:\n",
        "- фильтровать новые траектории, с которых будут взяты данные в $\\mathcal{D}_a$.\n",
        "- изменять пропорцию $k_e : k_a$. В начале в основном обучаться на экспертных данных.\n",
        "- для сбора датасета использовать микс распределений текущей и экспертной стратегий с помощью линейной интерполяции: $\\pi = \\alpha \\pi_e + (1-\\alpha)\\pi_a$. Начинать обучение с полностью экспертной стратегии и постепенно \"переносить вес\" на обученную стратегию, т.е. стартовать с $\\alpha = 1.0$ и в процессе обучения снижать $\\alpha \\rightarrow 0$.\n",
        "\n",
        "## SQIL (Soft Q Imitation Learning)\n",
        "\n",
        "[Статья](https://arxiv.org/abs/1905.11108).\n",
        "\n",
        "Задача: см. соотв. секцию в конце.\n",
        "\n",
        "Идея метода: SQIL преобразует задачу имитационного обучения в задачу обучения с подкреплением за счет введения специальной функции вознаграждения. Вместо того чтобы строить специальную функцию потерь для имитации (как в behavior cloning или DAgger), в SQIL:\n",
        "- Переходам из экспертных демонстраций назначается фиксированная положительная награда (например, +1).\n",
        "- Переходам агента, полученным в ходе его собственной активности, назначается значительно меньшая или нулевая награда.\n",
        "\n",
        "Агент обучается с помощью обычного off-policy алгоритма обучения с подкреплением (DQN, SAC и тп), используя модифицированную функцию награды.\n",
        "В результате агент стремится повторять поведение эксперта, потому что именно оно приводит к получению большей награды.\n",
        "\n",
        "Обозначения: $\\pi_a$ -- стратегия обучаемого агента, $\\pi_e$ -- стратегия эксперта, $\\mathcal{D}_e$ -- данные с эксперта.\n",
        "\n",
        "Алгоритм:\n",
        "1. Переходам из экспертного датасета $\\mathcal{D}_e = \\{(s_i, a_i, s'_i)\\}_{i=0}^N$ назначается вознаграждение $r = 1$.\n",
        "2. Инициализируется ReplayBuffer: $\\mathcal{D} = \\mathcal{D}_e$.\n",
        "3. Проводится раунд сбора новых траекторий $\\mathcal{D}_a = \\{(s_i, a_i, s'_i)\\}_{i=0}^M$ на основе **текущей** стратегии $\\pi_a$. Собранным траекториям назначается вознаграждение $r = 0$.\n",
        "3. Собранные данные добавляются в ReplayBuffer: $\\mathcal{D} = \\mathcal{D} \\cup \\mathcal{D}_a$.\n",
        "4. Проводится раунд обучения $\\pi_a$ на ReplayBuffer.\n",
        "5. Повторить с 3.  \n",
        "\n",
        "Таким образом, для метода SAC (soft actor-critic) надо изменить ReplayBuffer.\n",
        "\n",
        "Основные особенности метода:\n",
        "- Специальная функция награды;\n",
        "- Можно использовать любой стандартный off-policy алгоритм RL без модификаций;\n",
        "- Неявная имитация: агента не заставляют напрямую копировать эксперта — он сам стремится к стратегии, приносящей наибольшую отдачу.\n",
        "- Используется преимущество off-policy RL над имитационным обученим: агент, потенциально, может оптимизировать поведение эксперта.\n",
        "\n",
        "### Код\n",
        "Что взять за основу:\n",
        "- `SACAgent`;\n",
        "\n",
        "Что потребуется изменить:\n",
        "- `ReplayBuffer`;\n",
        "- метод `train`.\n",
        "\n",
        "Параметры:\n",
        "- $k_e : k_a$ -- пропорция, сколько в батче экспертных данных к данным с агента (обычно $1 : 1$). *NB: для этого потребуется хранить данные отдельно друг от друга.*\n",
        "\n",
        "# Задание\n",
        "\n",
        "1. Реализовать методы DAgger и SQIL.\n",
        "2. Сравнить три метода BC, DAgger и SQIL на трех заданиях в среде `PointMaze` (конфигурацию заданий см. ниже).\n",
        "3. [Optional] Изменить ReplayBuffer:\n",
        " - формировать батч таким образом, чтобы за одну эпоху последовательно и без повторений использовался каждый элемент датасета (1 эпоха = 1 раз пройти по всему датасету). Выводить число эпох.\n",
        "3. В отчете показать несколько графиков:\n",
        " - во время обучения -- график SR (процент успешно завершенных эпизодов при evaluate);\n",
        " - после обучения -- траектория движения обученного агента, SR при тестировании агента `n` раз на задании.\n",
        "4. В отчёте должны быть ссылки на проекты (public доступ) в WandB с логированием метрик экспериментов. По названию Run'а должно быть понятно, к какому эксперименту он относится, например: `task1_BC`, `task2_SQIL` и т.д.. Также в отчёте должен быть параграф с анализом результатов.\n",
        "\n"
      ],
      "metadata": {
        "id": "dWKc2DoN7gks"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Код"
      ],
      "metadata": {
        "id": "949GTIeLdGBe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Установка зависимостей для colab"
      ],
      "metadata": {
        "id": "n6mJMFGY7htM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    import google.colab\n",
        "    COLAB = True\n",
        "    if COLAB:\n",
        "        !pip -q install piglet\n",
        "        !pip -q install imageio_ffmpeg\n",
        "        !pip -q install moviepy==1.0.3\n",
        "        !pip install gymnasium[mujoco]\n",
        "        !pip install gymnasium_robotics\n",
        "        !pip install h5py\n",
        "except:\n",
        "    COLAB = False\n",
        "\n",
        "NEED_DRIVE = True\n",
        "HAS_DRIVE = False"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZqRL4Hnwqoak",
        "outputId": "f1250a1c-f423-4bc3-ea20-79e70155a070"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/67.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.5/67.5 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: gymnasium[mujoco] in /usr/local/lib/python3.11/dist-packages (1.1.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium[mujoco]) (2.0.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium[mujoco]) (3.1.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium[mujoco]) (4.13.1)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from gymnasium[mujoco]) (0.0.4)\n",
            "Collecting mujoco>=2.1.5 (from gymnasium[mujoco])\n",
            "  Downloading mujoco-3.3.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (44 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: imageio>=2.14.1 in /usr/local/lib/python3.11/dist-packages (from gymnasium[mujoco]) (2.37.0)\n",
            "Requirement already satisfied: pillow>=8.3.2 in /usr/local/lib/python3.11/dist-packages (from imageio>=2.14.1->gymnasium[mujoco]) (11.1.0)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from mujoco>=2.1.5->gymnasium[mujoco]) (1.4.0)\n",
            "Requirement already satisfied: etils[epath] in /usr/local/lib/python3.11/dist-packages (from mujoco>=2.1.5->gymnasium[mujoco]) (1.12.2)\n",
            "Collecting glfw (from mujoco>=2.1.5->gymnasium[mujoco])\n",
            "  Downloading glfw-2.8.0-py2.py27.py3.py30.py31.py32.py33.py34.py35.py36.py37.py38.p39.p310.p311.p312.p313-none-manylinux_2_28_x86_64.whl.metadata (5.4 kB)\n",
            "Requirement already satisfied: pyopengl in /usr/local/lib/python3.11/dist-packages (from mujoco>=2.1.5->gymnasium[mujoco]) (3.1.9)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from etils[epath]->mujoco>=2.1.5->gymnasium[mujoco]) (2025.3.2)\n",
            "Requirement already satisfied: importlib_resources in /usr/local/lib/python3.11/dist-packages (from etils[epath]->mujoco>=2.1.5->gymnasium[mujoco]) (6.5.2)\n",
            "Requirement already satisfied: zipp in /usr/local/lib/python3.11/dist-packages (from etils[epath]->mujoco>=2.1.5->gymnasium[mujoco]) (3.21.0)\n",
            "Downloading mujoco-3.3.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m58.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading glfw-2.8.0-py2.py27.py3.py30.py31.py32.py33.py34.py35.py36.py37.py38.p39.p310.p311.p312.p313-none-manylinux_2_28_x86_64.whl (243 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m243.4/243.4 kB\u001b[0m \u001b[31m21.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: glfw, mujoco\n",
            "Successfully installed glfw-2.8.0 mujoco-3.3.0\n",
            "Collecting gymnasium_robotics\n",
            "  Downloading gymnasium_robotics-1.3.1-py3-none-any.whl.metadata (8.7 kB)\n",
            "Collecting mujoco<3.2.0,>=2.2.0 (from gymnasium_robotics)\n",
            "  Downloading mujoco-3.1.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (44 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.3/44.3 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium_robotics) (2.0.2)\n",
            "Requirement already satisfied: gymnasium>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium_robotics) (1.1.1)\n",
            "Collecting PettingZoo>=1.23.0 (from gymnasium_robotics)\n",
            "  Downloading pettingzoo-1.24.3-py3-none-any.whl.metadata (8.5 kB)\n",
            "Requirement already satisfied: Jinja2>=3.0.3 in /usr/local/lib/python3.11/dist-packages (from gymnasium_robotics) (3.1.6)\n",
            "Requirement already satisfied: imageio in /usr/local/lib/python3.11/dist-packages (from gymnasium_robotics) (2.37.0)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium>=1.0.0->gymnasium_robotics) (3.1.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium>=1.0.0->gymnasium_robotics) (4.13.1)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from gymnasium>=1.0.0->gymnasium_robotics) (0.0.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from Jinja2>=3.0.3->gymnasium_robotics) (3.0.2)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from mujoco<3.2.0,>=2.2.0->gymnasium_robotics) (1.4.0)\n",
            "Requirement already satisfied: etils[epath] in /usr/local/lib/python3.11/dist-packages (from mujoco<3.2.0,>=2.2.0->gymnasium_robotics) (1.12.2)\n",
            "Requirement already satisfied: glfw in /usr/local/lib/python3.11/dist-packages (from mujoco<3.2.0,>=2.2.0->gymnasium_robotics) (2.8.0)\n",
            "Requirement already satisfied: pyopengl in /usr/local/lib/python3.11/dist-packages (from mujoco<3.2.0,>=2.2.0->gymnasium_robotics) (3.1.9)\n",
            "Requirement already satisfied: pillow>=8.3.2 in /usr/local/lib/python3.11/dist-packages (from imageio->gymnasium_robotics) (11.1.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from etils[epath]->mujoco<3.2.0,>=2.2.0->gymnasium_robotics) (2025.3.2)\n",
            "Requirement already satisfied: importlib_resources in /usr/local/lib/python3.11/dist-packages (from etils[epath]->mujoco<3.2.0,>=2.2.0->gymnasium_robotics) (6.5.2)\n",
            "Requirement already satisfied: zipp in /usr/local/lib/python3.11/dist-packages (from etils[epath]->mujoco<3.2.0,>=2.2.0->gymnasium_robotics) (3.21.0)\n",
            "Downloading gymnasium_robotics-1.3.1-py3-none-any.whl (26.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.1/26.1 MB\u001b[0m \u001b[31m74.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mujoco-3.1.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m108.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pettingzoo-1.24.3-py3-none-any.whl (847 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m847.8/847.8 kB\u001b[0m \u001b[31m48.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: PettingZoo, mujoco, gymnasium_robotics\n",
            "  Attempting uninstall: mujoco\n",
            "    Found existing installation: mujoco 3.3.0\n",
            "    Uninstalling mujoco-3.3.0:\n",
            "      Successfully uninstalled mujoco-3.3.0\n",
            "Successfully installed PettingZoo-1.24.3 gymnasium_robotics-1.3.1 mujoco-3.1.6\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.11/dist-packages (3.13.0)\n",
            "Requirement already satisfied: numpy>=1.19.3 in /usr/local/lib/python3.11/dist-packages (from h5py) (2.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if NEED_DRIVE and COLAB:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    HAS_DRIVE = True\n",
        "\n",
        "https://drive.google.com/file/d/1ODvl6a4N7Jw8Q-qF-zof8nLY1gMk1x3i/view?usp=drive_link"
      ],
      "metadata": {
        "id": "NQ1bcvxkrDGk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -O \"data_umaze_bad.hdf5\" --no-check-certificate \"https://drive.usercontent.google.com/download?id=1t_aAlPjniYUYL63CryEmhEJUyWkQBdCB&export=download&authuser=0\"\n",
        "!wget -O \"data_umaze_good.hdf5\" --no-check-certificate \"https://drive.usercontent.google.com/download?id=1hw5pbbyPUcffmVKM1FMdaDR93nEuqqsu&export=download&authuser=0\"\n",
        "!wget -O \"data_umaze_corner.hdf5\" --no-check-certificate \"https://drive.usercontent.google.com/download?id=1NqM7Xa6KKX6FCJNFaaxEKb2moRlU4Dg8&export=download&authuser=0\"\n",
        "!wget -O \"data_corner.hdf5\" --no-check-certificate \"https://drive.usercontent.google.com/download?id=1ODvl6a4N7Jw8Q-qF-zof8nLY1gMk1x3i&export=download&authuser=0\"\n",
        "!wget -O \"data_line.hdf5\" --no-check-certificate \"https://drive.usercontent.google.com/download?id=1FdGp66Ef8k2fkSdTghkoPti_NoGcOjSD&export=download&authuser=0\"\n",
        "!wget -O \"expert_Umaze.pt\" --no-check-certificate \"https://drive.usercontent.google.com/download?id=1daCX0dxBmAPtBAXAvk6s_BeSkndIBHLP&export=download&authuser=0\"\n",
        "!wget -O \"expert_medium.pt\" --no-check-certificate \"https://drive.usercontent.google.com/download?id=14g2motZ8KA_iUDcP_TSFjZFrZ4Zpe-6-&export=download&authuser=0\""
      ],
      "metadata": {
        "id": "ihqNNz4W7fhZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e17ce416-b4f7-4386-a32f-61574829d179"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-04-07 17:15:37--  https://drive.usercontent.google.com/download?id=1t_aAlPjniYUYL63CryEmhEJUyWkQBdCB&export=download&authuser=0\n",
            "Resolving drive.usercontent.google.com (drive.usercontent.google.com)... 142.251.2.132, 2607:f8b0:4023:c0d::84\n",
            "Connecting to drive.usercontent.google.com (drive.usercontent.google.com)|142.251.2.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 56506808 (54M) [application/octet-stream]\n",
            "Saving to: ‘data_umaze_bad.hdf5’\n",
            "\n",
            "data_umaze_bad.hdf5 100%[===================>]  53.89M  38.1MB/s    in 1.4s    \n",
            "\n",
            "2025-04-07 17:15:43 (38.1 MB/s) - ‘data_umaze_bad.hdf5’ saved [56506808/56506808]\n",
            "\n",
            "--2025-04-07 17:15:43--  https://drive.usercontent.google.com/download?id=1hw5pbbyPUcffmVKM1FMdaDR93nEuqqsu&export=download&authuser=0\n",
            "Resolving drive.usercontent.google.com (drive.usercontent.google.com)... 142.251.2.132, 2607:f8b0:4023:c0d::84\n",
            "Connecting to drive.usercontent.google.com (drive.usercontent.google.com)|142.251.2.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 56504774 (54M) [application/octet-stream]\n",
            "Saving to: ‘data_umaze_good.hdf5’\n",
            "\n",
            "data_umaze_good.hdf 100%[===================>]  53.89M  52.2MB/s    in 1.0s    \n",
            "\n",
            "2025-04-07 17:15:49 (52.2 MB/s) - ‘data_umaze_good.hdf5’ saved [56504774/56504774]\n",
            "\n",
            "--2025-04-07 17:15:50--  https://drive.usercontent.google.com/download?id=1NqM7Xa6KKX6FCJNFaaxEKb2moRlU4Dg8&export=download&authuser=0\n",
            "Resolving drive.usercontent.google.com (drive.usercontent.google.com)... 142.251.2.132, 2607:f8b0:4023:c0d::84\n",
            "Connecting to drive.usercontent.google.com (drive.usercontent.google.com)|142.251.2.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 55293640 (53M) [application/octet-stream]\n",
            "Saving to: ‘data_umaze_corner.hdf5’\n",
            "\n",
            "data_umaze_corner.h 100%[===================>]  52.73M  30.6MB/s    in 1.7s    \n",
            "\n",
            "2025-04-07 17:15:56 (30.6 MB/s) - ‘data_umaze_corner.hdf5’ saved [55293640/55293640]\n",
            "\n",
            "--2025-04-07 17:15:56--  https://drive.usercontent.google.com/download?id=1ODvl6a4N7Jw8Q-qF-zof8nLY1gMk1x3i&export=download&authuser=0\n",
            "Resolving drive.usercontent.google.com (drive.usercontent.google.com)... 142.251.2.132, 2607:f8b0:4023:c0d::84\n",
            "Connecting to drive.usercontent.google.com (drive.usercontent.google.com)|142.251.2.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 55313980 (53M) [application/octet-stream]\n",
            "Saving to: ‘data_corner.hdf5’\n",
            "\n",
            "data_corner.hdf5    100%[===================>]  52.75M  36.7MB/s    in 1.4s    \n",
            "\n",
            "2025-04-07 17:16:02 (36.7 MB/s) - ‘data_corner.hdf5’ saved [55313980/55313980]\n",
            "\n",
            "--2025-04-07 17:16:02--  https://drive.usercontent.google.com/download?id=1FdGp66Ef8k2fkSdTghkoPti_NoGcOjSD&export=download&authuser=0\n",
            "Resolving drive.usercontent.google.com (drive.usercontent.google.com)... 142.251.2.132, 2607:f8b0:4023:c0d::84\n",
            "Connecting to drive.usercontent.google.com (drive.usercontent.google.com)|142.251.2.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 63060469 (60M) [application/octet-stream]\n",
            "Saving to: ‘data_line.hdf5’\n",
            "\n",
            "data_line.hdf5      100%[===================>]  60.14M  35.5MB/s    in 1.7s    \n",
            "\n",
            "2025-04-07 17:16:09 (35.5 MB/s) - ‘data_line.hdf5’ saved [63060469/63060469]\n",
            "\n",
            "--2025-04-07 17:16:09--  https://drive.usercontent.google.com/download?id=1daCX0dxBmAPtBAXAvk6s_BeSkndIBHLP&export=download&authuser=0\n",
            "Resolving drive.usercontent.google.com (drive.usercontent.google.com)... 142.251.2.132, 2607:f8b0:4023:c0d::84\n",
            "Connecting to drive.usercontent.google.com (drive.usercontent.google.com)|142.251.2.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3041170 (2.9M) [application/octet-stream]\n",
            "Saving to: ‘expert_Umaze.pt’\n",
            "\n",
            "expert_Umaze.pt     100%[===================>]   2.90M  16.3MB/s    in 0.2s    \n",
            "\n",
            "2025-04-07 17:16:12 (16.3 MB/s) - ‘expert_Umaze.pt’ saved [3041170/3041170]\n",
            "\n",
            "--2025-04-07 17:16:12--  https://drive.usercontent.google.com/download?id=14g2motZ8KA_iUDcP_TSFjZFrZ4Zpe-6-&export=download&authuser=0\n",
            "Resolving drive.usercontent.google.com (drive.usercontent.google.com)... 142.251.2.132, 2607:f8b0:4023:c0d::84\n",
            "Connecting to drive.usercontent.google.com (drive.usercontent.google.com)|142.251.2.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3041068 (2.9M) [application/octet-stream]\n",
            "Saving to: ‘expert_medium.pt’\n",
            "\n",
            "expert_medium.pt    100%[===================>]   2.90M  --.-KB/s    in 0.1s    \n",
            "\n",
            "2025-04-07 17:16:16 (21.6 MB/s) - ‘expert_medium.pt’ saved [3041068/3041068]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Any, Callable, Dict, List, Optional, Tuple, Union\n",
        "\n",
        "from dataclasses import dataclass\n",
        "\n",
        "import os\n",
        "import random\n",
        "import time\n",
        "import h5py\n",
        "\n",
        "import numpy as np\n",
        "import gymnasium as gym\n",
        "import torch\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
        "from IPython.display import Image\n",
        "\n",
        "TensorBatch = List[torch.Tensor]\n",
        "\n",
        "try:\n",
        "    import wandb\n",
        "    WANDB_IS_AVAILABLE = True\n",
        "except ImportError:\n",
        "    WANDB_IS_AVAILABLE = False\n",
        "\n",
        "# os.environ[\"WANDB_API_KEY\"] = \"your wandb-api key\"\n",
        "os.environ[\"WANDB_API_KEY\"] = \"2493ada22b975bf0409f8240f642b7711e61be40\""
      ],
      "metadata": {
        "id": "mxWDcBNYd6Jq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Среда\n",
        "\n"
      ],
      "metadata": {
        "id": "ItzDcPCQgb7r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "PointMaze\n",
        "\n",
        "Среда для экспериментов: [**PointMaze**](https://robotics.farama.org/envs/maze/point_maze/).\n",
        "\n",
        "Задача поиска пути в лабиринте:\n",
        "- расположение препятствия фиксированное\n",
        "- начальная точка и точка цели задается в заранее определенной области\n",
        "- агент достиг своей цели, если расстояние между ним и целью < 0.5\n",
        "- конец эпизода: агент достиг своей цели или превышено число допустимых шагов в среде\n",
        "- награда: 1, если агент достиг цели\n",
        "- наблюдения: координаты агента, скорость агента, координаты цели $$obs = [x_a, y_a, v_x, v_y, x_g, y_g]$$\n",
        "- действие: линейная сила $$action = [F_x, F_y]$$\n",
        "- метрика: $SR = \\frac{success}{n_{ep}}$ -- процент успешно завершенных эпизодов. Агент решает задачу поиска пути от начальной точки до точки цели $n_{ep}$ раз. $success$ -- сколько раз цель была достигнута.\n"
      ],
      "metadata": {
        "id": "lnIvIRkNnQ-g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gymnasium_robotics\n",
        "\n",
        "gym.register_envs(gymnasium_robotics)\n",
        "\n",
        "\n",
        "class PointMazeArrayObservationWrapper(gym.ObservationWrapper):\n",
        "    def __init__(self, env):\n",
        "        super().__init__(env)\n",
        "        # Формируется векторное наблюдение из словаря\n",
        "        self._obs_keys = [\"observation\", \"desired_goal\"]\n",
        "        new_obs_len = np.sum(\n",
        "            [self.observation_space[k].shape[0] for k in self._obs_keys]\n",
        "        )\n",
        "        self.observation_space = gym.spaces.Box(\n",
        "            low=-np.inf, high=np.inf, shape=(new_obs_len,), dtype=np.float32\n",
        "        )\n",
        "\n",
        "    def observation(self, obs):\n",
        "        array_obs = np.concatenate([obs[key].flatten() for key in self._obs_keys])\n",
        "        return array_obs\n"
      ],
      "metadata": {
        "id": "tTFnypd6nGqu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Создание среды"
      ],
      "metadata": {
        "id": "6lp9LktIn04q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 'r' -- возможная начальная точка для агента, 'g' -- возможная целевая точка,\n",
        "# 'c' -- начальная или целевая точка\n",
        "TRAIN_MAP = [\n",
        "    [1, 1, 1, 1, 1],\n",
        "    [1, 0, \"r\", 0, 1],\n",
        "    [1, 1, 1, 0, 1],\n",
        "    [1, 0, \"g\", 0, 1],\n",
        "    [1, 1, 1, 1, 1],\n",
        "]\n",
        "\n",
        "pic_map = [\n",
        "    [1, 1, 1, 1, 1],\n",
        "    [1, 0, 0, 0, 1],\n",
        "    [1, 1, 1, 0, 1],\n",
        "    [1, 0, 0, 0, 1],\n",
        "    [1, 1, 1, 1, 1],\n",
        "]\n",
        "\n",
        "def make_env(seed):\n",
        "    def thunk():\n",
        "        env = gym.make(\n",
        "            \"PointMaze_UMaze-v3\",\n",
        "            max_episode_steps=200,\n",
        "            maze_map=TRAIN_MAP,\n",
        "            continuing_task=False,\n",
        "        )\n",
        "        env = PointMazeArrayObservationWrapper(env)\n",
        "        env = gym.wrappers.RecordEpisodeStatistics(env)\n",
        "        env.action_space.seed(seed)\n",
        "        return env\n",
        "    return thunk"
      ],
      "metadata": {
        "id": "hKAINeMunYnc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Отрисовка траекторий"
      ],
      "metadata": {
        "id": "-M0gGrqIn5KG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib\n",
        "matplotlib.use(\"Agg\")\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "\n",
        "def plot_walls(walls):\n",
        "    (height, width) = walls.shape\n",
        "    for i, j in zip(*np.where(walls)):\n",
        "        x = np.array([j, j + 1]) -width/2\n",
        "        y0 = np.array([i, i]) -height/2\n",
        "        y1 = np.array([i + 1, i + 1]) -height/2\n",
        "        plt.fill_between(x, y0, y1, color=\"grey\")\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "\n",
        "def plot_all_trajectories( walls=None, trajectories_=None, traj_filepath='output.png'):\n",
        "    plt.clf()\n",
        "    if walls  is not None:\n",
        "        plot_walls(walls)\n",
        "    if trajectories_ is not None:\n",
        "        all_points = trajectories_\n",
        "        sns.kdeplot(x=all_points[:, 0], y=all_points[:, 1], cmap='viridis', fill=True, alpha=0.5)\n",
        "    plt.savefig(traj_filepath)\n",
        "\n",
        "def plot_start_and_end_points(points, traj_filepath=\"start_and_end.png\"):\n",
        "    plt.clf()\n",
        "    plt.scatter(\n",
        "        points[:, 0],\n",
        "        points[:, 1],\n",
        "        marker=\".\",\n",
        "        color=\"red\",\n",
        "        s=200,\n",
        "        label=\"start\",\n",
        "    )\n",
        "    plt.scatter(\n",
        "        points[:, 2],\n",
        "        points[:, 3],\n",
        "        marker=\".\",\n",
        "        color=\"green\",\n",
        "        s=200,\n",
        "        label=\"end\",\n",
        "    )\n",
        "    plt.savefig(traj_filepath)\n",
        "\n",
        "\n",
        "def plot_one_trajectory(obs_vec, goal=None, walls=None, traj_filepath='output.png'):\n",
        "    plt.clf()\n",
        "    if walls  is not None:\n",
        "        plot_walls(walls)\n",
        "\n",
        "    if (obs_vec is not None):\n",
        "        obs_vec = np.array(obs_vec)\n",
        "        plt.plot(obs_vec[:, 0], obs_vec[:, 1], \".\", alpha=0.3)\n",
        "        plt.scatter(\n",
        "            [obs_vec[0, 0]],\n",
        "            [obs_vec[0, 1]],\n",
        "            marker=\"+\",\n",
        "            color=\"red\",\n",
        "            s=200,\n",
        "            label=\"start\",\n",
        "        )\n",
        "        plt.scatter(\n",
        "            [obs_vec[-1, 0]],\n",
        "            [obs_vec[-1, 1]],\n",
        "            marker=\"+\",\n",
        "            color=\"green\",\n",
        "            s=200,\n",
        "            label=\"end\",\n",
        "        )\n",
        "        if (goal is not None):\n",
        "            plt.scatter(\n",
        "                goal[:,0], goal[:,1], marker=\"*\", color=\"green\", s=200, label=\"goal\"\n",
        "            )\n",
        "    plt.savefig(traj_filepath)\n"
      ],
      "metadata": {
        "id": "BklMMgBbnubV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Тестирование агента"
      ],
      "metadata": {
        "id": "9WDn7ZgioFgQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@dataclass\n",
        "class EnvConfig:\n",
        "    num_eval_episodes: int = 10\n",
        "    eval_seed: int = 10\n",
        "\n",
        "class Evaluate:\n",
        "    def __init__(self, make_env, config):\n",
        "        self.num_episodes = config.num_eval_episodes\n",
        "        self.seed = config.eval_seed\n",
        "        self.env = make_env(self.seed)()\n",
        "        self.map_ = pic_map\n",
        "        self.map_ = np.array(self.map_)\n",
        "\n",
        "    def eval_actor(self, actor):\n",
        "        actor.eval()\n",
        "        success = []\n",
        "        for _ in range(self.num_episodes):\n",
        "            state, info = self.env.reset()\n",
        "            done, tr = False, False\n",
        "            while not (done or tr):\n",
        "                action = actor.get_action(state)\n",
        "                state, reward, done, tr, info = self.env.step(action)\n",
        "            if \"success\" in info:\n",
        "                success.append(info[\"success\"])\n",
        "            else:\n",
        "                success.append(info[\"episode\"][\"r\"])\n",
        "\n",
        "        actor.train()\n",
        "        return np.asarray(success).mean()\n",
        "\n",
        "    def eval_tr(self, actor, num_episodes=1, traj_filepath='output.png'):\n",
        "        actor.eval()\n",
        "        success = []\n",
        "        observations = []\n",
        "        for _ in range(num_episodes):\n",
        "            state, info = self.env.reset()\n",
        "            observations.append(state)\n",
        "            done, tr = False, False\n",
        "            while not (done or tr):\n",
        "                action = actor.get_action(state)\n",
        "                state, reward, done, tr, info = self.env.step(action)\n",
        "                observations.append(state)\n",
        "            if \"success\" in info:\n",
        "                success.append(info[\"success\"])\n",
        "            else:\n",
        "                success.append(info[\"episode\"][\"r\"])\n",
        "\n",
        "        actor.train()\n",
        "        print(f\"SR: {np.asarray(success).mean()}\")\n",
        "        plot_one_trajectory(observations, traj_filepath=traj_filepath, walls = self.map_)"
      ],
      "metadata": {
        "id": "nIRzza0ToIFz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Методы"
      ],
      "metadata": {
        "id": "VHEFioo3hXOv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Общее"
      ],
      "metadata": {
        "id": "8pHtTfZKhfQJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ReplayBuffer:\n",
        "    def __init__(\n",
        "        self,\n",
        "        state_dim: int,\n",
        "        action_dim: int,\n",
        "        buffer_size: int,\n",
        "        device: str = \"cpu\",\n",
        "        config=None,\n",
        "    ):\n",
        "        self._buffer_size = buffer_size\n",
        "        self._pointer = 0\n",
        "        self._size = 0\n",
        "        self._config = config\n",
        "\n",
        "        self._states = torch.zeros(\n",
        "            (buffer_size, state_dim), dtype=torch.float32, device=device\n",
        "        )\n",
        "        self._actions = torch.zeros(\n",
        "            (buffer_size, action_dim), dtype=torch.float32, device=device\n",
        "        )\n",
        "        self._rewards = torch.zeros(\n",
        "            (buffer_size, 1), dtype=torch.float32, device=device\n",
        "        )\n",
        "        self._next_states = torch.zeros(\n",
        "            (buffer_size, state_dim), dtype=torch.float32, device=device\n",
        "        )\n",
        "        self._dones = torch.zeros((buffer_size, 1), dtype=torch.float32, device=device)\n",
        "        self._device = device\n",
        "\n",
        "    def _to_tensor(self, data: np.ndarray) -> torch.Tensor:\n",
        "        return torch.tensor(data, dtype=torch.float32, device=self._device)\n",
        "\n",
        "    def load_dataset_from_file(self, path: str, with_goal: bool):\n",
        "        data = dict()\n",
        "        with h5py.File(path, \"r\") as f:\n",
        "            for key in f.keys():\n",
        "                if key == \"infos\":\n",
        "                    for info_key in f[key].keys():\n",
        "                        data[info_key] = f.get(key)[info_key][:]\n",
        "                    continue\n",
        "                data[key] = f.get(key)[:]\n",
        "        return data\n",
        "\n",
        "    def load_dataset(self, data: Dict[str, np.ndarray]):\n",
        "        # Для offline методов: загрузка датасета\n",
        "        if self._size != 0:\n",
        "            raise ValueError(\"Trying to load data into non-empty replay buffer\")\n",
        "        n_transitions = data[\"observations\"].shape[0]\n",
        "        if n_transitions > self._buffer_size:\n",
        "            raise ValueError(\n",
        "                \"Replay buffer is smaller than the dataset you are trying to load!\"\n",
        "            )\n",
        "        self._states[:n_transitions] = self._to_tensor(data[\"observations\"])\n",
        "        self._actions[:n_transitions] = self._to_tensor(data[\"actions\"])\n",
        "        self._rewards[:n_transitions] = self._to_tensor(data[\"rewards\"][..., None])\n",
        "        self._next_states[:n_transitions] = self._to_tensor(data[\"next_observations\"])\n",
        "        self._dones[:n_transitions] = self._to_tensor(data[\"terminals\"][..., None])\n",
        "        self._size += n_transitions\n",
        "        self._pointer = min(self._size, n_transitions)\n",
        "\n",
        "        print(f\"Dataset size: {n_transitions}\")\n",
        "\n",
        "    def sample(self, batch_size: int) -> TensorBatch:\n",
        "        indices = np.random.randint(0, self._size, size=batch_size)\n",
        "        states = self._states[indices]\n",
        "        actions = self._actions[indices]\n",
        "        rewards = self._rewards[indices]\n",
        "        next_states = self._next_states[indices]\n",
        "        dones = self._dones[indices]\n",
        "        return [states, actions, rewards, next_states, dones]\n",
        "\n",
        "    def add_transition(\n",
        "        self,\n",
        "        state: np.ndarray,\n",
        "        action: np.ndarray,\n",
        "        reward: float,\n",
        "        next_state: np.ndarray,\n",
        "        done: bool,\n",
        "    ):\n",
        "        # Для off-policy методов: добавление данных при взаимодействии агента со средой\n",
        "        self._states[self._pointer] = self._to_tensor(state)\n",
        "        self._actions[self._pointer] = self._to_tensor(action)\n",
        "        self._rewards[self._pointer] = self._to_tensor(reward)\n",
        "        self._next_states[self._pointer] = self._to_tensor(next_state)\n",
        "        self._dones[self._pointer] = self._to_tensor(done)\n",
        "\n",
        "        self._pointer = (self._pointer + 1) % self._buffer_size\n",
        "        self._size = min(self._size + 1, self._buffer_size)\n"
      ],
      "metadata": {
        "id": "31JfYGrFrgTP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Logger:\n",
        "    def __init__(self, args):\n",
        "        run_name = f\"{args.env_id}__{args.exp_name}__{args.seed}__{int(time.time())}\"\n",
        "        if args.track:\n",
        "            import wandb\n",
        "\n",
        "            self.run = wandb.init(\n",
        "                project=args.wandb_project_name,\n",
        "                entity=args.wandb_entity,\n",
        "                sync_tensorboard=True,\n",
        "                config=vars(args),\n",
        "                name=run_name,\n",
        "                monitor_gym=True,\n",
        "                save_code=True,\n",
        "            )\n",
        "        else:\n",
        "            self.run = None\n",
        "        self.writer = SummaryWriter(f\"runs/{run_name}\")\n",
        "        self.writer.add_text(\n",
        "            \"hyperparameters\",\n",
        "            \"|param|value|\\n|-|-|\\n%s\"\n",
        "            % (\"\\n\".join([f\"|{key}|{value}|\" for key, value in vars(args).items()])),\n",
        "        )\n",
        "\n",
        "    def init_time(self):\n",
        "        self.start_time = time.time()\n",
        "\n",
        "    def log_metric(self, metric_dict: dict, global_step: int):\n",
        "        for key, item in metric_dict.items():\n",
        "            self.writer.add_scalar(key, item, global_step)\n",
        "\n",
        "    def time_step(self, global_step):\n",
        "        if global_step % 100 == 0:\n",
        "            print(\"SPS:\", int(global_step / (time.time() - self.start_time)))\n",
        "            self.log_metric(\n",
        "                {\"charts/SPS\": int(global_step / (time.time() - self.start_time))},\n",
        "                global_step,\n",
        "            )\n",
        "\n",
        "    def close(self):\n",
        "        self.writer.close()\n",
        "        if self.run:\n",
        "            self.run.finish()\n"
      ],
      "metadata": {
        "id": "6bUVT5LErnH2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_offpolicy(env, agent, rb, config, logger, eval_):\n",
        "    if config.checkpoints_path is not None:\n",
        "        print(f\"Checkpoints path: {config.checkpoints_path}\")\n",
        "        os.makedirs(config.checkpoints_path, exist_ok=True)\n",
        "\n",
        "    # start train\n",
        "    obs, _ = env.reset(seed=config.seed)\n",
        "    for global_step in range(config.total_timesteps):\n",
        "        # выбор действий\n",
        "        if global_step < config.learning_starts:\n",
        "            actions = np.array(env.action_space.sample())\n",
        "        else:\n",
        "            actions = agent.get_action(obs)\n",
        "\n",
        "        # выполнение действия в среде\n",
        "        next_obs, rewards, terminations, truncations, infos = env.step(actions)\n",
        "\n",
        "        # логирование информации\n",
        "        if terminations or truncations:\n",
        "            if (infos is not None) and (\"episode\" in infos):\n",
        "                print(\n",
        "                    f\"global_step={global_step}, episodic_return={infos['episode']['r']}\"\n",
        "                )\n",
        "                metrics = {\n",
        "                    \"charts/episodic_return\": infos[\"episode\"][\"r\"],\n",
        "                    \"charts/episodic_length\": infos[\"episode\"][\"l\"],\n",
        "                }\n",
        "                if \"success\" in infos:\n",
        "                    metrics[\"charts/success\"] = infos[\"success\"]\n",
        "                logger.log_metric(metrics, global_step)\n",
        "\n",
        "        # сохранение данных в ReplayBuffer\n",
        "        real_next_obs = next_obs.copy()\n",
        "        rb.add_transition(obs, actions, rewards, real_next_obs, terminations)\n",
        "\n",
        "        if terminations or truncations:\n",
        "            next_obs, _ = env.reset()\n",
        "        obs = next_obs\n",
        "\n",
        "        # обучение\n",
        "        if global_step > config.learning_starts:\n",
        "            data = rb.sample(config.batch_size)\n",
        "            metrics = agent.update(data, global_step)\n",
        "\n",
        "            # логгирование\n",
        "            if global_step % config.eval_freq == 0:\n",
        "                SR = eval_.eval_actor(agent)\n",
        "                metrics[\"SR\"] = SR\n",
        "                logger.log_metric(metrics, global_step)\n",
        "                logger.time_step(global_step)\n",
        "                print(f\"Step {global_step}: SR = {SR}\")\n",
        "                if config.checkpoints_path:\n",
        "                    torch.save(\n",
        "                        agent.state_dict(),\n",
        "                        os.path.join(\n",
        "                            config.checkpoints_path, f\"checkpoint_{global_step}.pt\"\n",
        "                        ),\n",
        "                    )\n",
        "\n",
        "\n",
        "def train_offline(env, agent, rb, config, logger, eval_):\n",
        "    if config.checkpoints_path is not None:\n",
        "        print(f\"Checkpoints path: {config.checkpoints_path}\")\n",
        "        os.makedirs(config.checkpoints_path, exist_ok=True)\n",
        "    # данные загружаются в ReplayBuffer\n",
        "    dataset = rb.load_dataset_from_file(path=config.data_path, with_goal=False)\n",
        "    rb.load_dataset(dataset)\n",
        "\n",
        "    # start train\n",
        "    obs, _ = env.reset(seed=config.seed)\n",
        "    for global_step in range(config.total_timesteps):\n",
        "        # обучение\n",
        "        data = rb.sample(config.batch_size)\n",
        "        metrics = agent.update(data, global_step)\n",
        "\n",
        "        # логгирование\n",
        "        if global_step % config.eval_freq == 0:\n",
        "            SR = eval_.eval_actor(agent)\n",
        "            metrics[\"SR\"] = SR\n",
        "            logger.log_metric(metrics, global_step)\n",
        "            logger.time_step(global_step)\n",
        "            print(f\"Step {global_step}: SR = {SR}\")\n",
        "            if config.checkpoints_path:\n",
        "                torch.save(\n",
        "                    agent.state_dict(),\n",
        "                    os.path.join(\n",
        "                        config.checkpoints_path, f\"checkpoint_{global_step}.pt\"\n",
        "                    ),\n",
        "                )\n",
        "\n",
        "\n",
        "def train(config, Agent, ReplayBuffer, train_func, make_env):\n",
        "    logger = Logger(config)\n",
        "\n",
        "    # seeding\n",
        "    random.seed(config.seed)\n",
        "    np.random.seed(config.seed)\n",
        "    torch.manual_seed(config.seed)\n",
        "    torch.backends.cudnn.deterministic = config.torch_deterministic\n",
        "\n",
        "    device = torch.device(\n",
        "        \"cuda\" if torch.cuda.is_available() and config.cuda else \"cpu\"\n",
        "    )\n",
        "\n",
        "    # среда\n",
        "    env = make_env(config.seed)()\n",
        "    eval_ = Evaluate(make_env, config)\n",
        "\n",
        "    observation_space_shape = env.observation_space.shape\n",
        "    action_space = env.action_space\n",
        "    env.observation_space.dtype = np.float32\n",
        "\n",
        "    # агент\n",
        "    agent = Agent(config, device, observation_space_shape, action_space)\n",
        "\n",
        "    rb = ReplayBuffer(\n",
        "        buffer_size=config.buffer_size,\n",
        "        state_dim=env.observation_space.shape[0],\n",
        "        action_dim=env.action_space.shape[0],\n",
        "        device=device,\n",
        "        config=config,\n",
        "    )\n",
        "\n",
        "    logger.init_time()\n",
        "\n",
        "    # обучение\n",
        "    train_func(env, agent, rb, config, logger, eval_)\n",
        "\n",
        "    env.close()\n",
        "    logger.close()\n",
        "    return agent"
      ],
      "metadata": {
        "id": "ZQz2NzrqrsYa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "LOG_STD_MAX = 2\n",
        "LOG_STD_MIN = -5\n",
        "\n",
        "\n",
        "class Actor(nn.Module):\n",
        "    def __init__(self, observation_space_shape, action_space):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(np.array(observation_space_shape).prod(), 256)\n",
        "        self.fc2 = nn.Linear(256, 256)\n",
        "        self.fc_mean = nn.Linear(256, np.prod(action_space.shape))\n",
        "        self.fc_logstd = nn.Linear(256, np.prod(action_space.shape))\n",
        "        # action rescaling\n",
        "        self.register_buffer(\n",
        "            \"action_scale\",\n",
        "            torch.tensor(\n",
        "                (action_space.high - action_space.low) / 2.0,\n",
        "                dtype=torch.float32,\n",
        "            ),\n",
        "        )\n",
        "        self.register_buffer(\n",
        "            \"action_bias\",\n",
        "            torch.tensor(\n",
        "                (action_space.high + action_space.low) / 2.0,\n",
        "                dtype=torch.float32,\n",
        "            ),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        mean = self.fc_mean(x)\n",
        "        log_std = self.fc_logstd(x)\n",
        "        log_std = torch.tanh(log_std)\n",
        "        log_std = LOG_STD_MIN + 0.5 * (LOG_STD_MAX - LOG_STD_MIN) * (\n",
        "            log_std + 1\n",
        "        )  # From SpinUp / Denis Yarats\n",
        "\n",
        "        return mean, log_std\n",
        "\n",
        "    def get_action(self, x):\n",
        "        mean, log_std = self(x)\n",
        "        std = log_std.exp()\n",
        "        normal = torch.distributions.Normal(mean, std)\n",
        "        x_t = normal.rsample()  # for reparameterization trick (mean + std * N(0,1))\n",
        "        y_t = torch.tanh(x_t)\n",
        "        action = y_t * self.action_scale + self.action_bias\n",
        "        log_prob = normal.log_prob(x_t)\n",
        "        # Enforcing Action Bound\n",
        "        log_prob -= torch.log(self.action_scale * (1 - y_t.pow(2)) + 1e-6)\n",
        "        log_prob = log_prob.sum()\n",
        "        mean = torch.tanh(mean) * self.action_scale + self.action_bias\n",
        "        return action, log_prob, mean\n",
        "\n",
        "class SoftQNetwork(nn.Module):\n",
        "    def __init__(self, observation_space_shape, action_space):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(\n",
        "            np.array(observation_space_shape).prod() + np.prod(action_space.shape),\n",
        "            256,\n",
        "        )\n",
        "        self.fc2 = nn.Linear(256, 256)\n",
        "        self.fc3 = nn.Linear(256, 1)\n",
        "\n",
        "    def forward(self, x, a):\n",
        "        x = torch.cat([x, a], -1)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "EXP_ADV_MAX = 100.0\n",
        "\n",
        "\n",
        "class Squeeze(nn.Module):\n",
        "    def __init__(self, dim=-1):\n",
        "        super().__init__()\n",
        "        self.dim = dim\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        return x.squeeze(dim=self.dim)\n",
        "\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        dims,\n",
        "        activation_fn: Callable[[], nn.Module] = nn.ReLU,\n",
        "        output_activation_fn: Callable[[], nn.Module] = None,\n",
        "        squeeze_output: bool = False,\n",
        "        dropout: Optional[float] = None,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        n_dims = len(dims)\n",
        "        if n_dims < 2:\n",
        "            raise ValueError(\"MLP requires at least two dims (input and output)\")\n",
        "\n",
        "        layers = []\n",
        "        for i in range(n_dims - 2):\n",
        "            layers.append(nn.Linear(dims[i], dims[i + 1]))\n",
        "            layers.append(activation_fn())\n",
        "\n",
        "            if dropout is not None:\n",
        "                layers.append(nn.Dropout(dropout))\n",
        "\n",
        "        layers.append(nn.Linear(dims[-2], dims[-1]))\n",
        "        if output_activation_fn is not None:\n",
        "            layers.append(output_activation_fn())\n",
        "        if squeeze_output:\n",
        "            if dims[-1] != 1:\n",
        "                raise ValueError(\"Last dim must be 1 when squeezing\")\n",
        "            layers.append(Squeeze(-1))\n",
        "        self.net = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        return self.net(x)\n",
        "\n",
        "\n",
        "class ValueFunction(nn.Module):\n",
        "    def __init__(self, state_dim: int, hidden_dim: int = 256, n_hidden: int = 2):\n",
        "        super().__init__()\n",
        "        dims = [state_dim, *([hidden_dim] * n_hidden), 1]\n",
        "        self.v = MLP(dims, squeeze_output=True)\n",
        "\n",
        "    def forward(self, state: torch.Tensor) -> torch.Tensor:\n",
        "        return self.v(state)"
      ],
      "metadata": {
        "id": "hPKDK4wKsUAz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Общая конфигурация для экспериментов\n",
        "@dataclass\n",
        "class ConfigExp:\n",
        "    exp_name: str = \"offline_rl\"\n",
        "    \"\"\"the name of this experiment\"\"\"\n",
        "    seed: int = 1\n",
        "    \"\"\"seed of the experiment\"\"\"\n",
        "    torch_deterministic: bool = True\n",
        "    \"\"\"if toggled, `torch.backends.cudnn.deterministic=False`\"\"\"\n",
        "    cuda: bool = True\n",
        "    \"\"\"if toggled, cuda will be enabled by default\"\"\"\n",
        "    track: bool = True\n",
        "    \"\"\"if toggled, this experiment will be tracked with Weights and Biases\"\"\"\n",
        "    wandb_project_name: str = \"offline-rl-hw\"\n",
        "    \"\"\"the wandb's project name\"\"\"\n",
        "    wandb_entity: str = None\n",
        "    \"\"\"the entity (team) of wandb's project\"\"\"\n",
        "    capture_video: bool = False\n",
        "    \"\"\"whether to capture videos of the agent performances (check out `videos` folder)\"\"\"\n",
        "\n",
        "    env_id: str = \"PointMaze_UMaze-v3\"\n",
        "    \"\"\"the environment id of the task\"\"\"\n",
        "    total_timesteps: int = 300000\n",
        "    \"\"\"total timesteps of the experiments\"\"\"\n",
        "    num_envs: int = 1\n",
        "    \"\"\"the number of parallel game environments\"\"\"\n",
        "    buffer_size: int = int(1e6)\n",
        "    \"\"\"the replay memory buffer size\"\"\"\n",
        "\n",
        "    eval_freq: int = int(5e3)\n",
        "    checkpoints_path: str = \"model\"\n",
        "    eval_seed: int = 1024\n",
        "    num_eval_episodes: int = 10"
      ],
      "metadata": {
        "id": "-rmj5nPArxPM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Behavioral Cloning"
      ],
      "metadata": {
        "id": "330LSqoAh7BK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@dataclass\n",
        "class ConfigBC(ConfigExp):\n",
        "    batch_size: int = 1024\n",
        "    policy_lr: float = 3e-4\n",
        "    data_path: str = \"/content/data_umaze_dense.hdf5\""
      ],
      "metadata": {
        "id": "5UQHcI0Ysikx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# data =\n",
        "        #           obs: torch.Tensor,\n",
        "        #           action: torch.Tensor,\n",
        "        #           reward: torch.Tensor,\n",
        "        #           next_obs: torch.Tensor,\n",
        "        #           done: torch.Tensor,\n",
        "\n",
        "class BCAgent(nn.Module):\n",
        "    def __init__(self, cfg: ConfigBC, device, observation_space_shape, action_space):\n",
        "        super().__init__()\n",
        "        self.cfg = cfg\n",
        "        self.device = device\n",
        "        self.init_actor(observation_space_shape, action_space)\n",
        "\n",
        "    def init_actor(self, observation_space_shape, action_space):\n",
        "        self.actor = Actor(observation_space_shape, action_space).to(self.device)\n",
        "        self.actor_optimizer = optim.Adam(\n",
        "            list(self.actor.parameters()), lr=self.cfg.policy_lr\n",
        "        )\n",
        "\n",
        "    def update(\n",
        "        self,\n",
        "        data,\n",
        "        global_step,\n",
        "    ) -> dict:\n",
        "        # update actor\n",
        "        observations = data[0]\n",
        "        action = data[1]\n",
        "\n",
        "        pi, log_pi, _ = self.actor.get_action(observations)\n",
        "\n",
        "        # actor_loss = ...\n",
        "        ####### Код с семинара ########\n",
        "        actor_loss = F.mse_loss(pi, action)\n",
        "        ##############################\n",
        "\n",
        "        self.actor_optimizer.zero_grad()\n",
        "        actor_loss.backward()\n",
        "        self.actor_optimizer.step()\n",
        "\n",
        "        actor_metrics = {\"losses/actor_loss\": actor_loss.item()}\n",
        "        return actor_metrics\n",
        "\n",
        "    def get_action(self, observation: np.ndarray, epsilon: float = 0.0) -> int:\n",
        "        actions, _, _ = self.actor.get_action(torch.Tensor(observation).to(self.device))\n",
        "        actions = actions.detach().cpu().numpy()\n",
        "        return actions\n",
        "\n",
        "    def state_dict(self) -> Dict[str, Any]:\n",
        "        return {\n",
        "            \"actor\": self.actor.state_dict(),\n",
        "            \"actor_optim\": self.actor_optimizer.state_dict(),\n",
        "        }\n",
        "\n",
        "    def load_state_dict(self, state_dict: Dict[str, Any]):\n",
        "        self.actor.load_state_dict(state_dict=state_dict[\"actor\"])\n",
        "        self.actor_optimizer.load_state_dict(state_dict=state_dict[\"actor_optim\"])\n"
      ],
      "metadata": {
        "id": "Q5ufKXzWsjW7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### SAC"
      ],
      "metadata": {
        "id": "orFDQCVch9Hn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@dataclass\n",
        "class ConfigSAC(ConfigExp):\n",
        "    gamma: float = 0.99\n",
        "    \"\"\"the discount factor gamma\"\"\"\n",
        "    tau: float = 0.005\n",
        "    \"\"\"target smoothing coefficient (default: 0.005)\"\"\"\n",
        "    batch_size: int = 1024\n",
        "    \"\"\"the batch size of sample from the reply memory\"\"\"\n",
        "    learning_starts: int = 5e3\n",
        "    \"\"\"timestep to start learning\"\"\"\n",
        "    policy_lr: float = 3e-4\n",
        "    \"\"\"the learning rate of the policy network optimizer\"\"\"\n",
        "    q_lr: float = 1e-3\n",
        "    \"\"\"the learning rate of the Q network network optimizer\"\"\"\n",
        "    policy_frequency: int = 2\n",
        "    \"\"\"the frequency of training policy (delayed)\"\"\"\n",
        "    target_network_frequency: int = 1  # Denis Yarats' implementation delays this by 2.\n",
        "    \"\"\"the frequency of updates for the target nerworks\"\"\"\n",
        "    alpha: float = 0.2\n",
        "    \"\"\"Entropy regularization coefficient.\"\"\"\n",
        "    autotune: bool = True\n",
        "    \"\"\"automatic tuning of the entropy coefficient\"\"\"\n",
        "\n",
        "    data_path: str = \"notebook/data_umaze_d.hdf5\""
      ],
      "metadata": {
        "id": "3A98YTOmsoWW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SACAgent(nn.Module):\n",
        "    def __init__(self, cfg: ConfigSAC, device, observation_space_shape, action_space):\n",
        "        super().__init__()\n",
        "        self.cfg = cfg\n",
        "        self.device = device\n",
        "        self.init_actor_critic(observation_space_shape, action_space)\n",
        "\n",
        "    def init_actor_critic(self, observation_space_shape, action_space):\n",
        "        self.actor = Actor(observation_space_shape, action_space).to(self.device)\n",
        "        self.qf1 = SoftQNetwork(observation_space_shape, action_space).to(self.device)\n",
        "        self.qf2 = SoftQNetwork(observation_space_shape, action_space).to(self.device)\n",
        "        self.qf1_target = SoftQNetwork(observation_space_shape, action_space).to(\n",
        "            self.device\n",
        "        )\n",
        "        self.qf2_target = SoftQNetwork(observation_space_shape, action_space).to(\n",
        "            self.device\n",
        "        )\n",
        "        self.qf1_target.load_state_dict(self.qf1.state_dict())\n",
        "        self.qf2_target.load_state_dict(self.qf2.state_dict())\n",
        "        self.q_optimizer = optim.Adam(\n",
        "            list(self.qf1.parameters()) + list(self.qf2.parameters()), lr=self.cfg.q_lr\n",
        "        )\n",
        "        self.actor_optimizer = optim.Adam(\n",
        "            list(self.actor.parameters()), lr=self.cfg.policy_lr\n",
        "        )\n",
        "\n",
        "        # Automatic entropy tuning\n",
        "        if self.cfg.autotune:\n",
        "            self.target_entropy = -torch.prod(\n",
        "                torch.Tensor(action_space.shape).to(self.device)\n",
        "            ).item()\n",
        "            self.log_alpha = torch.zeros(1, requires_grad=True, device=self.device)\n",
        "            self.alpha = self.log_alpha.exp().item()\n",
        "            self.a_optimizer = optim.Adam([self.log_alpha], lr=self.cfg.q_lr)\n",
        "        else:\n",
        "            self.alpha = self.cfg.alpha\n",
        "\n",
        "    def actor_loss(self, pi, log_pi, data):\n",
        "        observations = data[0]\n",
        "        # actor_loss = mean(- (Q' + a H))\n",
        "        ####### Здесь ваш код ########\n",
        "        qf1_pi = self.qf1(observations, pi)\n",
        "        qf2_pi = self.qf2(observations, pi)\n",
        "        min_qf_pi = torch.min(qf1_pi, qf2_pi)\n",
        "        actor_loss = ((self.alpha * log_pi) - min_qf_pi).mean()\n",
        "        ##############################\n",
        "        return actor_loss\n",
        "\n",
        "    def compute_critic_loss(\n",
        "        self,\n",
        "        data,\n",
        "    ) -> Tuple[torch.Tensor, dict, dict]:\n",
        "        observations, actions, rewards, next_observations, dones = data\n",
        "        rewards = rewards.flatten()\n",
        "        dones = dones.flatten()\n",
        "        with torch.no_grad():\n",
        "            next_state_actions, next_state_log_pi, _ = self.actor.get_action(\n",
        "                next_observations\n",
        "            )\n",
        "            # для s', a' вычислить Q(s', a') с помощью уравнения Беллмана\n",
        "            #         1) min_qf_next_target = Q' + a H\n",
        "            #         2) next_q_value = Q(s', a') = r + (1 - d) * gamma * min_qf_next_target\n",
        "            ####### Код с семинара ########\n",
        "            qf1_next_target = self.qf1_target(next_observations, next_state_actions)\n",
        "            qf2_next_target = self.qf2_target(next_observations, next_state_actions)\n",
        "            min_qf_next_target = (\n",
        "                torch.min(qf1_next_target, qf2_next_target)\n",
        "                - self.alpha * next_state_log_pi\n",
        "            )\n",
        "            next_q_value = rewards + (1 - dones) * self.cfg.gamma * (min_qf_next_target).view(-1)\n",
        "            ##############################\n",
        "\n",
        "        qf1_a_values = self.qf1(observations, actions).view(-1)\n",
        "        qf2_a_values = self.qf2(observations, actions).view(-1)\n",
        "        qf1_loss = F.mse_loss(qf1_a_values, next_q_value)\n",
        "        qf2_loss = F.mse_loss(qf2_a_values, next_q_value)\n",
        "        qf_loss = qf1_loss + qf2_loss\n",
        "        metrics = {\n",
        "            \"losses/qf1_values\": qf1_a_values.mean().item(),\n",
        "            \"losses/qf2_values\": qf2_a_values.mean().item(),\n",
        "            \"losses/qf1_loss\": qf1_loss.item(),\n",
        "            \"losses/qf2_loss\": qf2_loss.item(),\n",
        "            \"losses/qf_loss\": qf_loss.item() / 2.0,\n",
        "        }\n",
        "        return qf_loss, metrics\n",
        "\n",
        "    def update_critic(\n",
        "        self,\n",
        "        data,\n",
        "    ) -> dict:\n",
        "        qf_loss, metrics = self.compute_critic_loss(data)\n",
        "\n",
        "        # optimize the model\n",
        "        self.q_optimizer.zero_grad()\n",
        "        qf_loss.backward()\n",
        "        self.q_optimizer.step()\n",
        "\n",
        "        return metrics\n",
        "\n",
        "    def update_target_critic(self):\n",
        "        for param, target_param in zip(\n",
        "            self.qf1.parameters(), self.qf1_target.parameters()\n",
        "        ):\n",
        "            target_param.data.copy_(\n",
        "                self.cfg.tau * param.data + (1 - self.cfg.tau) * target_param.data\n",
        "            )\n",
        "        for param, target_param in zip(\n",
        "            self.qf2.parameters(), self.qf2_target.parameters()\n",
        "        ):\n",
        "            target_param.data.copy_(\n",
        "                self.cfg.tau * param.data + (1 - self.cfg.tau) * target_param.data\n",
        "            )\n",
        "\n",
        "    def update_alpha_loss(self, data):\n",
        "        observations = data[0]\n",
        "        metrics = dict()\n",
        "        if self.cfg.autotune:\n",
        "            with torch.no_grad():\n",
        "                _, log_pi, _ = self.actor.get_action(observations)\n",
        "            alpha_loss = (-self.log_alpha.exp() * (log_pi + self.target_entropy)).mean()\n",
        "\n",
        "            self.a_optimizer.zero_grad()\n",
        "            alpha_loss.backward()\n",
        "            self.a_optimizer.step()\n",
        "            self.alpha = self.log_alpha.exp().item()\n",
        "            metrics[\"losses/alpha_loss\"] = alpha_loss.item()\n",
        "        metrics[\"losses/alpha\"] = self.alpha\n",
        "        return metrics\n",
        "\n",
        "\n",
        "    def update_actor(self, data):\n",
        "        observations = data[0]\n",
        "\n",
        "        pi, log_pi, _ = self.actor.get_action(observations)\n",
        "\n",
        "        actor_loss = self.actor_loss(pi, log_pi, data)\n",
        "\n",
        "        self.actor_optimizer.zero_grad()\n",
        "        actor_loss.backward()\n",
        "        self.actor_optimizer.step()\n",
        "\n",
        "        metrics = {\"losses/actor_loss\": actor_loss.item()}\n",
        "        return metrics\n",
        "\n",
        "    def update(\n",
        "        self,\n",
        "        data,\n",
        "        global_step,\n",
        "    ) -> dict:\n",
        "        # update critic\n",
        "        metrics = self.update_critic(data)\n",
        "\n",
        "        # update actor\n",
        "        if global_step % self.cfg.policy_frequency == 0:\n",
        "            for _ in range(self.cfg.policy_frequency):\n",
        "                actor_metrics = self.update_actor(data)\n",
        "                alpha_metrics = self.update_alpha_loss(data)\n",
        "            metrics.update(actor_metrics)\n",
        "            metrics.update(alpha_metrics)\n",
        "\n",
        "        # update the target networks\n",
        "        if global_step % self.cfg.target_network_frequency == 0:\n",
        "            self.update_target_critic()\n",
        "        return metrics\n",
        "\n",
        "    def state_dict(self) -> Dict[str, Any]:\n",
        "        res =  {\n",
        "            \"actor\": self.actor.state_dict(),\n",
        "            \"qf1\": self.qf1.state_dict(),\n",
        "            \"qf2\": self.qf2.state_dict(),\n",
        "            \"qf1_target\": self.qf1_target.state_dict(),\n",
        "            \"qf2_target\": self.qf2_target.state_dict(),\n",
        "            \"q_optimizer\": self.q_optimizer.state_dict(),\n",
        "            \"actor_optim\": self.actor_optimizer.state_dict(),\n",
        "        }\n",
        "        if self.cfg.autotune:\n",
        "            res[\"sac_log_alpha\"] =  self.log_alpha\n",
        "            res[\"sac_log_alpha_optim\"] =  self.a_optimizer.state_dict()\n",
        "\n",
        "        return res\n",
        "\n",
        "    def load_state_dict(self, state_dict: Dict[str, Any]):\n",
        "        self.actor.load_state_dict(state_dict=state_dict[\"actor\"])\n",
        "        self.qf1.load_state_dict(state_dict=state_dict[\"qf1\"])\n",
        "        self.qf2.load_state_dict(state_dict=state_dict[\"qf2\"])\n",
        "        self.qf1_target.load_state_dict(state_dict=state_dict[\"qf1_target\"])\n",
        "        self.qf2_target.load_state_dict(state_dict=state_dict[\"qf2_target\"])\n",
        "\n",
        "        self.q_optimizer.load_state_dict(state_dict=state_dict[\"q_optimizer\"])\n",
        "        self.actor_optimizer.load_state_dict(state_dict=state_dict[\"actor_optim\"])\n",
        "\n",
        "        if self.cfg.autotune:\n",
        "            self.log_alpha = state_dict[\"sac_log_alpha\"]\n",
        "            self.a_optimizer.load_state_dict(state_dict=state_dict[\"sac_log_alpha_optim\"])\n",
        "\n",
        "    def get_action(self, observation: np.ndarray, epsilon: float = 0.0) -> int:\n",
        "        actions, _, _ = self.actor.get_action(torch.Tensor(observation).to(self.device))\n",
        "        actions = actions.detach().cpu().numpy()\n",
        "        return actions\n"
      ],
      "metadata": {
        "id": "pwE4E5ZDsts9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Задания для проверки методов"
      ],
      "metadata": {
        "id": "DM4opa8JMJIv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Задание 1 - простое\n",
        "Два датасета из семинара.\n",
        "\n",
        "Замечание. Для метода SQIL:\n",
        "- параметр `total_timestep = 400000`.\n",
        "- SR может быть долгое время равен `SR = 0`, а потом резко стать `SR = 1`."
      ],
      "metadata": {
        "id": "BPCYwEGrMbvd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "TRAIN_MAP = [\n",
        "    [1, 1, 1, 1, 1],\n",
        "    [1, 0, \"r\", 0, 1],\n",
        "    [1, 1, 1, 0, 1],\n",
        "    [1, 0, \"g\", 0, 1],\n",
        "    [1, 1, 1, 1, 1],\n",
        "]\n",
        "pic_map = [\n",
        "    [1, 1, 1, 1, 1],\n",
        "    [1, 0, 0, 0, 1],\n",
        "    [1, 1, 1, 0, 1],\n",
        "    [1, 0, 0, 0, 1],\n",
        "    [1, 1, 1, 1, 1],\n",
        "]\n",
        "total_timesteps = 1000\n",
        "data_path_1=\"/content/data_umaze_good.hdf5\"\n",
        "data_path_2=\"/content/data_umaze_bad.hdf5\"\n",
        "expert_path=\"/content/expert_Umaze.pt\""
      ],
      "metadata": {
        "id": "kvNXgh8a5Pmc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Задание 2\n",
        "Повернуть за угол\n"
      ],
      "metadata": {
        "id": "hoLM2L6EUgTx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "TRAIN_MAP = [\n",
        "    [1, 1, 1, 1, 1],\n",
        "    [1, \"r\", 0, 0, 1],\n",
        "    [1, 1, 1, 0, 1],\n",
        "    [1, 0, 0, \"g\", 1],\n",
        "    [1, 1, 1, 1, 1],\n",
        "]\n",
        "pic_map = [\n",
        "    [1, 1, 1, 1, 1],\n",
        "    [1, 0, 0, 0, 1],\n",
        "    [1, 1, 1, 0, 1],\n",
        "    [1, 0, 0, 0, 1],\n",
        "    [1, 1, 1, 1, 1],\n",
        "]\n",
        "total_timesteps = 400000\n",
        "data_path=\"/content/data_umaze_corner.hdf5\"\n",
        "expert_path=\"/content/expert_Umaze.pt\""
      ],
      "metadata": {
        "id": "6MVJ5urdQlvY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "TRAIN_MAP = [\n",
        "                [1, 1, 1, 1, 1, 1, 1],\n",
        "                [1, 0, 0, 0, 0, \"r\", 1],\n",
        "                [1, 0, 1, 0, 1, 1, 1],\n",
        "                [1, 1, 0, \"g\", 1, 1, 1],\n",
        "                [1, 1, 1, 1, 1, 1, 1],\n",
        "            ]\n",
        "pic_map = [\n",
        "                [1, 1, 1, 1, 1, 1, 1],\n",
        "                [1, 1, 0, 0, 1, 1, 1],\n",
        "                [1, 0, 1, 0, 1, 1, 1],\n",
        "                [1, 0, 0, 0, 0, 0, 1],\n",
        "                [1, 1, 1, 1, 1, 1, 1],\n",
        "            ]\n",
        "total_timesteps = 400000\n",
        "data_path=\"/content/data_corner.hdf5\"\n",
        "expert_path=\"/content/expert_medium.pt\""
      ],
      "metadata": {
        "id": "MsrwoJseNXEC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Задание 3\n",
        "Пройти по прямой."
      ],
      "metadata": {
        "id": "v65LamXlMr7c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "TRAIN_MAP = [\n",
        "                [1, 1, 1, 1, 1, 1, 1],\n",
        "                [1, \"r\", 0, 0, \"g\", 0, 1],\n",
        "                [1, 0, 1, 0, 1, 1, 1],\n",
        "                [1, 1, 0, 0, 1, 1, 1],\n",
        "                [1, 1, 1, 1, 1, 1, 1],\n",
        "            ]\n",
        "pic_map = [\n",
        "                [1, 1, 1, 1, 1, 1, 1],\n",
        "                [1, 1, 0, 0, 1, 1, 1],\n",
        "                [1, 0, 1, 0, 1, 1, 1],\n",
        "                [1, 0, 0, 0, 0, 0, 1],\n",
        "                [1, 1, 1, 1, 1, 1, 1],\n",
        "            ]\n",
        "total_timesteps = 400000\n",
        "data_path=\"/content/data_line.hdf5\"\n",
        "expert_path=\"/content/expert_medium.pt\""
      ],
      "metadata": {
        "id": "cgPbilaBVoCG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Пример запуска"
      ],
      "metadata": {
        "id": "a7-6V8F66-bA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Данные"
      ],
      "metadata": {
        "id": "GKfD8RH_iFU4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Загрузка\n",
        "import numpy as np\n",
        "import h5py\n",
        "\n",
        "load_data = dict()\n",
        "with h5py.File(\"data_umaze_good.hdf5\", \"r\") as f:\n",
        "    for key in f.keys():\n",
        "        if key == \"infos\":\n",
        "            for info_key in f[key].keys():\n",
        "                load_data[info_key] = f.get(key)[info_key][:]\n",
        "            continue\n",
        "        load_data[key] = f.get(key)[:]"
      ],
      "metadata": {
        "id": "Sc5JT2pAiMXz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for key, val in load_data.items():\n",
        "    print(f\"{key}: {val.shape} {val.dtype}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GUw3vn8kiH7k",
        "outputId": "52224804-b589-4b4f-9c4c-0d101b80e327"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "actions: (500006, 2) float32\n",
            "next_observations: (500006, 6) float64\n",
            "observations: (500006, 6) float64\n",
            "rewards: (500006,) float64\n",
            "terminals: (500006,) bool\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pic_map = [\n",
        "    [1, 1, 1, 1, 1],\n",
        "    [1, 0, 0, 0, 1],\n",
        "    [1, 1, 1, 0, 1],\n",
        "    [1, 0, 0, 0, 1],\n",
        "    [1, 1, 1, 1, 1],\n",
        "]\n",
        "plot_one_trajectory(load_data[\"observations\"], traj_filepath=\"data.png\", walls=np.array(pic_map))\n",
        "Image(filename='data.png')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 497
        },
        "id": "AyKMZxv2iSbM",
        "outputId": "c9d956fd-3aaa-4eb4-c8be-5399cb0aa485"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {},
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Обучение"
      ],
      "metadata": {
        "id": "eCdPebIw7Er2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "TRAIN_MAP = [\n",
        "    [1, 1, 1, 1, 1],\n",
        "    [1, 0, \"r\", 0, 1],\n",
        "    [1, 1, 1, 0, 1],\n",
        "    [1, 0, \"g\", 0, 1],\n",
        "    [1, 1, 1, 1, 1],\n",
        "]\n",
        "\n",
        "config = ConfigBC(data_path=\"/content/data_umaze_good.hdf5\",\n",
        "                  total_timesteps=1000, eval_freq= int(1e2))\n",
        "agent_BC = train(config, BCAgent, ReplayBuffer, train_offline, make_env)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "y9vutnok4lM-",
        "outputId": "90aabfde-4c83-4c76-eb67-b19bbe26491c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>SR</td><td>▁▁▁▁██▁▁▅▁▅▅▁</td></tr><tr><td>charts/SPS</td><td>▁▆▇▇█████████</td></tr><tr><td>global_step</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>losses/actor_loss</td><td>█▂▂▁▁▁▁▂▁▁▂▂▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>SR</td><td>0</td></tr><tr><td>charts/SPS</td><td>162</td></tr><tr><td>global_step</td><td>12000</td></tr><tr><td>losses/actor_loss</td><td>0.21231</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">PointMaze_UMaze-v3__offline_rl__1__1744045146</strong> at: <a href='https://wandb.ai/nesterova/offline-rl-hw/runs/b3d0r5mc' target=\"_blank\">https://wandb.ai/nesterova/offline-rl-hw/runs/b3d0r5mc</a><br> View project at: <a href='https://wandb.ai/nesterova/offline-rl-hw' target=\"_blank\">https://wandb.ai/nesterova/offline-rl-hw</a><br>Synced 5 W&B file(s), 0 media file(s), 71 artifact file(s) and 1 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250407_165906-b3d0r5mc/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.9"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250407_170829-ejz2eggt</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/nesterova/offline-rl-hw/runs/ejz2eggt' target=\"_blank\">PointMaze_UMaze-v3__offline_rl__1__1744045709</a></strong> to <a href='https://wandb.ai/nesterova/offline-rl-hw' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/nesterova/offline-rl-hw' target=\"_blank\">https://wandb.ai/nesterova/offline-rl-hw</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/nesterova/offline-rl-hw/runs/ejz2eggt' target=\"_blank\">https://wandb.ai/nesterova/offline-rl-hw/runs/ejz2eggt</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m When using several event log directories, please call `wandb.tensorboard.patch(root_logdir=\"...\")` before `wandb.init`\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checkpoints path: model\n",
            "Dataset size: 500006\n",
            "SPS: 0\n",
            "Step 0: SR = 0.0\n",
            "SPS: 21\n",
            "Step 100: SR = 0.7\n",
            "SPS: 29\n",
            "Step 200: SR = 0.7\n",
            "SPS: 36\n",
            "Step 300: SR = 1.0\n",
            "SPS: 42\n",
            "Step 400: SR = 1.0\n",
            "SPS: 47\n",
            "Step 500: SR = 1.0\n",
            "SPS: 51\n",
            "Step 600: SR = 1.0\n",
            "SPS: 54\n",
            "Step 700: SR = 1.0\n",
            "SPS: 56\n",
            "Step 800: SR = 1.0\n",
            "SPS: 58\n",
            "Step 900: SR = 1.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>SR</td><td>▁▆▆███████</td></tr><tr><td>charts/SPS</td><td>▁▄▅▅▆▇▇███</td></tr><tr><td>global_step</td><td>▁▁▁▂▂▂▃▃▃▃▃▃▄▄▄▅▅▅▆▆▆▆▆▆▇▇▇███</td></tr><tr><td>losses/actor_loss</td><td>█▂▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>SR</td><td>1</td></tr><tr><td>charts/SPS</td><td>58</td></tr><tr><td>global_step</td><td>900</td></tr><tr><td>losses/actor_loss</td><td>0.05834</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">PointMaze_UMaze-v3__offline_rl__1__1744045709</strong> at: <a href='https://wandb.ai/nesterova/offline-rl-hw/runs/ejz2eggt' target=\"_blank\">https://wandb.ai/nesterova/offline-rl-hw/runs/ejz2eggt</a><br> View project at: <a href='https://wandb.ai/nesterova/offline-rl-hw' target=\"_blank\">https://wandb.ai/nesterova/offline-rl-hw</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250407_170829-ejz2eggt/logs</code>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Оценка"
      ],
      "metadata": {
        "id": "ZAzP_ijb7I66"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pic_map = [\n",
        "    [1, 1, 1, 1, 1],\n",
        "    [1, 0, 0, 0, 1],\n",
        "    [1, 1, 1, 0, 1],\n",
        "    [1, 0, 0, 0, 1],\n",
        "    [1, 1, 1, 1, 1],\n",
        "]\n",
        "\n",
        "eval = Evaluate(make_env, EnvConfig())\n",
        "eval.eval_tr(agent_BC)\n",
        "Image(filename='output.png')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 514
        },
        "id": "JzVOsHWR7IK_",
        "outputId": "606ca80f-c554-42f3-df8c-22be5529ed6e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SR: 1.0\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {},
          "execution_count": 87
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Загрузка весов эксперта. Эксперт - это агент, обученный с помощью метода SAC для конкретной карты. Может построить путь для любых начальных и целевых точек."
      ],
      "metadata": {
        "id": "GSM0zjDi7MFA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "env = make_env(1)()\n",
        "observation_space_shape = env.observation_space.shape\n",
        "action_space = env.action_space\n",
        "device = torch.device(\n",
        "        \"cuda\" if torch.cuda.is_available() and config.cuda else \"cpu\"\n",
        "    )\n",
        "\n",
        "\n",
        "config_expert = ConfigSAC()\n",
        "expert_agent = SACAgent(config_expert, device, observation_space_shape, action_space)\n",
        "expert_agent.load_state_dict(torch.load(\"/content/expert_Umaze.pt\", map_location=device))"
      ],
      "metadata": {
        "id": "oPkEzx3Z7K2b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eval.eval_tr(expert_agent)\n",
        "Image(filename='output.png')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 514
        },
        "id": "aVPurbS_mro4",
        "outputId": "155004d8-8883-4e8a-8a26-df30a6531cc7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SR: 1.0\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {},
          "execution_count": 92
        }
      ]
    }
  ]
}