# Домашнее задание №2. Имитационное обучение и оффлайн обучение с подкреплением

## Теоретическое задание

Теоретическое задание состоит из части на алгоритм TD($\lambda$) и части на методы аппроксимации стратегии.

**Задание 1.** Доказать, что если вычислять обновления весов на каждом шаге (без их применения), то суммарное обновление онлайнового и оффлайного варианта одинаково.

*NB: Под оффлайновым обновлением здесь подразумевается оффлайновый алгоритм $\lambda$-отдачи. Под онлайновым обновлением подразумевается алгоритм TD($\lambda$). Подсказки можно найти в заданиях в конце главы 12.1 книги Саттона и Барто.*

**Задание 2.** Доказать равнозначность оффлайного алгоритма $\lambda$-отдачи и истинно онлайнового алгоритма TD($\lambda$).

*См. соответствующую главу 12.5 книги Саттона и Барто и статью [van Seijen et al., 2016](https://www.jmlr.org/papers/volume17/15-599/15-599.pdf).*

**Задание 3.** Воспользуйтесь своими знаниями о клеточном мире и его динамике, чтобы найти точное символьное выражение для оптимальной вероятности выбора действия `right` в примере 13.1 книги Саттона и Барто.

## Практическое задание

Практическое задание посвящено темам imitation learning и offline RL. Тетрадка с заданием представлена [в репозитории](imitation_learning_offline_rl.ipynb) и в [Google Colab](https://colab.research.google.com/drive/1V53JfJ0xzlaHG4LX_y5T3vhgL4sW0ffd).

## Формат сдачи

Сдача осуществляется путем создания Pull Request'а. *NB: в GitHub Classroom PR будет создан автоматически, когда вы запушите первый коммит*. Убедитесь, что:

- в PR есть и на видном месте финальная версия вашего кода: ссылкой на колаб и/или коммитом тетради.
- в коммитах нет лишнего мусора, output в ячейках ноутбука не требует бесконечно скроллить.
- на видном месте и в удобной для просмотра форме сам отчет (pdf, markdown, отчет в wandb и тп)
- на видном месте ссылка на логи запусков в wandb.
- текст читаемый, если теоретическое задание сдается письменно от руки (т.е. сканы/фото, объединенные в pdf)
